RAG-con-LLMS
============

DescripciÃ³n del Proyecto
------------------------

![img.png](src/img.png)

Este proyecto implementa una soluciÃ³n de tipo RAG (retrieved augmented generation) que permite interactuar con un modelo de lenguaje (LLM) a travÃ©s de una API para generar respuestas a preguntas basadas en un documento proporcionado. Utiliza embeddings para dividir el documento en chunks y almacenar estos en una base de datos vectorial para realizar bÃºsquedas de similaridad y proporcionar contexto relevante al LLM.

CaracterÃ­sticas
---------------


-   API: Desarrollada en FastAPI para interactuar con el LLM.
-   LLM: Utiliza un modelo de Cohere para generar respuestas, .
-   Modelo de Embeddings: Divide el documento en chunks y los almacena en una base de datos vectorial utilizando ChromaDB. Modelo utilizado : "embed-multilingual-v3.0"
-   CachÃ©: Implementa una cachÃ© simple para almacenar y recuperar respuestas generadas previamente.

Estructura del Proyecto
-----------------------

```plaintext
.
â”œâ”€â”€ app
â”‚   â”œâ”€â”€ api
â”‚   â”‚   â””â”€â”€ endpoints.py        # Contiene los endpoints de la API.
â”‚   â”œâ”€â”€ cache
â”‚   â”‚   â””â”€â”€ cache.py            # Implementa la clase SimpleCache para manejo de cachÃ©.
â”‚   â”œâ”€â”€ core
â”‚   â”‚   â””â”€â”€ config.py           # Archivo de configuraciÃ³n centralizado.
â”‚   â”‚   â””â”€â”€ model.py            # InicializaciÃ³n del modelo y embeddings.
â”‚   â”œâ”€â”€ database
â”‚   â”‚   â””â”€â”€ setup.py            # Script para configurar y generar la base de datos.
â”‚   â”‚   â””â”€â”€ chroma              # Contiene la base de datos Chroma generada.
â”‚   â”œâ”€â”€ models
â”‚   â”‚   â””â”€â”€ request_models.py   # Modelos de solicitud usando Pydantic.
â”‚   â”œâ”€â”€ utils
â”‚   â”‚   â””â”€â”€ language_detection.py  # Funciones utilitarias para detecciÃ³n de idioma.
â”‚   â”‚   â””â”€â”€ text_processing.py     # Funciones de procesamiento de texto.
â”‚   â””â”€â”€ main.py                 # Punto de entrada principal de la aplicaciÃ³n FastAPI.
â”œâ”€â”€ data
â”‚   â””â”€â”€ documento.docx          # Documentos de datos.
â”œâ”€â”€ postman
â”‚   â””â”€â”€ RAG API Collection pi data.postman_collection.json  # ColecciÃ³n de Postman para pruebas de la API.
â”œâ”€â”€ Dockerfile                  # Archivo para construir la imagen Docker.
â””â”€â”€ requirements.txt            # Archivo de dependencias.`
```

InstalaciÃ³n y ConfiguraciÃ³n
---------------------------

### Prerrequisitos

-   Python 3.10
-   [Docker](https://www.docker.com/)
-   [Postman](https://www.postman.com/downloads/)

### InstalaciÃ³n

1.  Clona el repositorio:

    ```sh
    git clone https://github.com/tu_usuario/RAG-con-LLMS.git
    cd RAG-con-LLMS
    ```
2.  Crea un entorno virtual e instala las dependencias:

    ```sh
    python -m venv venv
    source venv/bin/activate  # En Windows: venv\Scripts\activate
    pip install -r requirements.txt
    ```



3.  Configura las variables de entorno:

    -   Abre `app/core/config.py` y establece tu clave API de Cohere:

        ```python
        COHERE_API_KEY = "tu_clave_api"
        ```
### ConfiguraciÃ³n de la Base de Datos

1.  Ejecuta el script para configurar la base de datos:

   ``` sh
    python app/database/setup.py
   ```

### EjecuciÃ³n de la API

1.  Inicia la API:

    ```sh
    uvicorn app.main:app --host 0.0.0.0 --port 8000
    ```

### Uso de Docker

1.  Construye la imagen Docker:

    ```sh
    docker build -t rag-con-llms .
    ```

2.  Inicia un contenedor desde la imagen:

    ```sh
    docker run -p 8000:8000 rag-con-llms
    ```

Uso de la API
-------------

### Endpoints Disponibles

-   POST /ask: EnvÃ­a una pregunta al LLM y obtiene una respuesta basada en el documento proporcionado.

#### Ejemplo de Solicitud

```json
{
  "user_name": "John Doe",
  "question": "Â¿QuiÃ©n es Zara?"
}
```

#### Ejemplo de Respuesta

```json
{
  "response": "Zara es un personaje mencionado en el documento. ðŸŒŸ"
}
```

Pruebas de la API con Postman
-----------------------------

### ConfiguraciÃ³n de Postman

1.  Descarga e instala [Postman](https://www.postman.com/downloads/).
2.  Inicia el Postman Desktop Agent y asegÃºrate de que estÃ© corriendo.
3.  Importa la colecciÃ³n de Postman:
    -   Abre Postman y haz clic en "Import" en la parte superior izquierda.
    -   Selecciona el archivo `RAG API Collection pi data.postman_collection.json` desde la carpeta `postman`.
4.  Configura la colecciÃ³n:
    -   AsegÃºrate de que la API estÃ© corriendo localmente en `http://localhost:8000`.
5.  Ejecuta las solicitudes:
    -   Selecciona la solicitud "Ask Question" en la colecciÃ³n y haz clic en "Send" para enviar la solicitud y ver la respuesta.
    -   TambiÃ©n puedes ejecutar toda la colecciÃ³n utilizando el Collection Runner de Postman para ejecutar mÃºltiples solicitudes en secuencia.

Consideraciones
---------------

-   AsegÃºrate de que la API key de Cohere estÃ© correctamente configurada en `app/core/config.py`.
-   Verifica que la base de datos se haya creado correctamente en `app/database/chroma` antes de iniciar la API.
-   Utiliza Postman para probar los endpoints y asegurarte de que las respuestas sean correctas y consistentes.
